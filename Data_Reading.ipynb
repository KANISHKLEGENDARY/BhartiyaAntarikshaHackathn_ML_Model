{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "594f5200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Timestamp  PM2.5 (µg/m³)\n",
      "5    2019-01-01 05:00:00         260.00\n",
      "29   2019-01-02 05:00:00         283.00\n",
      "53   2019-01-03 05:00:00         298.00\n",
      "77   2019-01-04 05:00:00         230.00\n",
      "101  2019-01-05 05:00:00         295.00\n",
      "...                  ...            ...\n",
      "8645 2019-12-27 05:00:00         193.00\n",
      "8669 2019-12-28 05:00:00         307.50\n",
      "8693 2019-12-29 05:00:00         253.75\n",
      "8717 2019-12-30 05:00:00         306.50\n",
      "8741 2019-12-31 05:00:00         137.50\n",
      "\n",
      "[365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Extracting PM2.5 levels from the CPCB data\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV\n",
    "pm_df = pd.read_csv(\"CPCB Data/City_wise_raw_data_1Hr_2019_Faridabad_1Hr.csv\")\n",
    "\n",
    "# Parse Timestamp column\n",
    "pm_df['Timestamp'] = pd.to_datetime(pm_df['Timestamp'], errors='coerce')\n",
    "\n",
    "# Create a date range for 1st to 18th June\n",
    "date_range = pd.date_range(start='2019-01-01 05:00:00', end='2019-12-31 05:00:00', freq='D')\n",
    "\n",
    "# Filter for only 05:00 data in that date range\n",
    "pm_filtered = pm_df[pm_df['Timestamp'].isin(date_range)]\n",
    "\n",
    "# Keep only Timestamp and PM2.5 columns\n",
    "pm_filtered = pm_filtered[['Timestamp', 'PM2.5 (µg/m³)']]\n",
    "\n",
    "# Optional: Sort by date\n",
    "pm_filtered = pm_filtered.sort_values('Timestamp')\n",
    "\n",
    "# Print the results\n",
    "print(pm_filtered)\n",
    "\n",
    "# Save for training\n",
    "pm_filtered.to_csv(\"pm25_cpcb_05AM_01janto31dec_2019.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c35a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files combined successfully!\n"
     ]
    }
   ],
   "source": [
    "#Combining the two csv files\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"combined_pm2.5_para.csv\")\n",
    "df2 = pd.read_csv(\"aod_data.csv\")\n",
    "combined_df = pd.concat([df1,df2], ignore_index= True)\n",
    "combined_df.to_csv(\"combined_pm2.5_para.csv\", index=False)\n",
    "print(\"Files combined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57eac91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full structure of the HDF5 file:\n",
      "Dataset: AOD — shape: (1, 551, 551)\n",
      "Dataset: latitude — shape: (551,)\n",
      "Dataset: longitude — shape: (551,)\n",
      "Dataset: time — shape: (1,)\n"
     ]
    }
   ],
   "source": [
    "#Trying to identify the shape of AOD, Latitude, and Longitude\n",
    "import h5py\n",
    "\n",
    "file_path = \"AOD Data/3DIMG_15JUN2024_0530_L2G_AOD_V02R00.h5\"\n",
    "\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    print(\"Full structure of the HDF5 file:\")\n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"Dataset: {name} — shape: {obj.shape}\")\n",
    "    f.visititems(print_structure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6baab5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date    Mean_AOD\n",
      "0  2024-06-01 -999.000000\n",
      "1  2024-06-02 -776.836670\n",
      "2  2024-06-03 -776.832703\n",
      "3  2024-06-04 -999.000000\n",
      "4  2024-06-05 -776.539917\n",
      "5  2024-06-06 -776.747070\n",
      "6  2024-06-07 -776.795715\n",
      "7  2024-06-08 -999.000000\n",
      "8  2024-06-09 -776.817932\n",
      "9  2024-06-10 -776.841675\n",
      "10 2024-06-11 -887.868042\n",
      "11 2024-06-12 -776.887695\n",
      "12 2024-06-13 -776.807800\n",
      "13 2024-06-14 -776.628784\n",
      "14 2024-06-15 -776.783203\n",
      "15 2024-06-16 -776.845398\n",
      "16 2024-06-17 -776.820801\n",
      "17 2024-06-18 -776.784424\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder where your HDF5 files are stored\n",
    "folder_path = \"AOD Data/\"\n",
    "\n",
    "# Define Faridabad region bounds\n",
    "lat_min, lat_max = 28.2, 28.5\n",
    "lon_min, lon_max = 77.2, 77.5\n",
    "\n",
    "# Initialize list to store results\n",
    "results = []\n",
    "\n",
    "# List all .h5 files in the folder\n",
    "for filename in sorted(os.listdir(folder_path)):\n",
    "    if filename.endswith(\".h5\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                # Load datasets\n",
    "                aod = f['AOD'][:].squeeze()\n",
    "                lat = f['latitude'][:]\n",
    "                lon = f['longitude'][:]\n",
    "                \n",
    "                # Flip for correct orientation\n",
    "                lat_flipped = lat[::-1]\n",
    "                aod_flipped = aod[::-1, :]\n",
    "                lon2d, lat2d = np.meshgrid(lon, lat_flipped)\n",
    "\n",
    "                # Apply mask for Faridabad region\n",
    "                mask = (lat2d >= lat_min) & (lat2d <= lat_max) & (lon2d >= lon_min) & (lon2d <= lon_max)\n",
    "                aod_faridabad_values = aod_flipped[mask]\n",
    "                mean_aod = np.nanmean(aod_faridabad_values)\n",
    "\n",
    "                # Extract date from filename (assumes format: 3DIMG_15JUN2024_0530_....h5)\n",
    "                date_str = filename.split(\"_\")[1]\n",
    "                date = pd.to_datetime(date_str, format='%d%b%Y')\n",
    "\n",
    "                # Append to results\n",
    "                results.append({'Date': date, 'Mean_AOD': mean_aod})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {e}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "df.sort_values('Date', inplace=True)\n",
    "\n",
    "# Save to CSV for training\n",
    "df.to_csv(\"mean_aod_faridabad.csv\", index=False)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a06249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Mean_AOD           Timestamp  PM2.5\n",
      "0 2024-06-01 -999.0000 2024-06-01 05:00:00  76.40\n",
      "1 2024-06-02 -776.8367 2024-06-02 05:00:00  74.98\n",
      "2 2024-06-03 -776.8327 2024-06-03 05:00:00  77.86\n",
      "3 2024-06-04 -999.0000 2024-06-04 05:00:00  55.40\n",
      "4 2024-06-05 -776.5399 2024-06-05 05:00:00  88.99\n"
     ]
    }
   ],
   "source": [
    "#Combine AOD and PM2.5 into One Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "aod_df = pd.read_csv(\"mean_aod_faridabad.csv\")\n",
    "pm_df = pd.read_csv(\"pm25_cpcb_05AM_1to18june.csv\")\n",
    "\n",
    "# Convert date columns to datetime\n",
    "aod_df['Date'] = pd.to_datetime(aod_df['Date'])\n",
    "pm_df['Timestamp'] = pd.to_datetime(pm_df['Timestamp'])\n",
    "\n",
    "# Extract just the date part but KEEP datetime64[ns] type\n",
    "pm_df['Date'] = pm_df['Timestamp'].dt.normalize()\n",
    "\n",
    "# Merge on Date\n",
    "combined_df = pd.merge(aod_df, pm_df, left_on='Date', right_on='Date')\n",
    "\n",
    "# Rename for simplicity\n",
    "combined_df.rename(columns={\"PM2.5 (µg/m³)\": \"PM2.5\"}, inplace=True)\n",
    "\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53cb4c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error on test set: 16.329789183384506\n",
      "Predicted PM2.5 for 2024-06-19 at 05:00: 59.31425022570035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91931\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Training a simple linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Features and target\n",
    "X = combined_df[['Mean_AOD']]  # Feature\n",
    "y = combined_df['PM2.5']       # Target\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Mean Absolute Error on test set:\", mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "#Predicting PM2.5 for 19th June\n",
    "aod_19_june = 0.97  # Replace this with your actual computed value\n",
    "predicted_pm = model.predict([[aod_19_june]])\n",
    "print(\"Predicted PM2.5 for 2024-06-19 at 05:00:\", predicted_pm[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db348329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Evaluation:\n",
      "MAE: 7.778910714285711\n",
      "RMSE: 8.834053260522328\n",
      "R² Score: 0.5988373888613501\n",
      "\n",
      "🔮 Predicted PM2.5 for 19 June 2024 at 05:00: 77.30559999999991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91931\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Prediction by Random Forest\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Load and merge the datasets\n",
    "aod_df = pd.read_csv(\"mean_aod_faridabad.csv\")\n",
    "pm_df = pd.read_csv(\"pm25_cpcb_05AM_1to18june.csv\")\n",
    "\n",
    "# Convert to datetime\n",
    "aod_df['Date'] = pd.to_datetime(aod_df['Date'])\n",
    "pm_df['Timestamp'] = pd.to_datetime(pm_df['Timestamp'])\n",
    "pm_df['Date'] = pm_df['Timestamp'].dt.normalize()\n",
    "\n",
    "# Merge on 'Date'\n",
    "combined_df = pd.merge(aod_df, pm_df, on='Date')\n",
    "combined_df.rename(columns={\"PM2.5 (µg/m³)\": \"PM2.5\"}, inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "X = combined_df[['Mean_AOD']]\n",
    "y = combined_df['PM2.5']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"✅ Model Evaluation:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R² Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "#Prediction for 19th June\n",
    "aod_19_june = 0.97\n",
    "\n",
    "predicted_pm25 = rf.predict([[aod_19_june]])\n",
    "print(\"\\n🔮 Predicted PM2.5 for 19 June 2024 at 05:00:\", predicted_pm25[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9b975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation:\n",
      "MAE: 28.026991176470588\n",
      "RMSE: 36.08490578960144\n",
      "R²: 0.0041481018634580424\n",
      "\n",
      "🔮 Predicted PM2.5 for 11 May 2024 at 05:30 IST: 96.84259999999999\n",
      "💾 Prediction saved to 'predicted_pm25.csv'\n"
     ]
    }
   ],
   "source": [
    "#Combining MOSDAC and CPCB data with MERRA and predicting the PM2.5 Level\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from netCDF4 import Dataset, num2date\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "# ---------- Step 1: Load INSAT AOD from CSV ----------\n",
    "aod_df = pd.read_csv(\"aod_data.csv\")\n",
    "aod_df['Date'] = pd.to_datetime(aod_df['Date'])  # 🔄 Convert to datetime for merging\n",
    "# print(aod_df.head())\n",
    "\n",
    "# ---------- Step 2: Load CPCB PM2.5 ----------\n",
    "pm_df = pd.read_csv(\"combined_pm2.5_para.csv\")\n",
    "pm_df['Timestamp'] = pd.to_datetime(pm_df['Timestamp'])\n",
    "pm_df['Date'] = pm_df['Timestamp'].dt.normalize()\n",
    "\n",
    "# ---------- Step 3: Load MERRA .nc4 files ----------\n",
    "def extract_merra_features(nc_folder):\n",
    "    records = []\n",
    "    for file in os.listdir(nc_folder):\n",
    "        if file.endswith(\".nc\"):\n",
    "            ds = Dataset(os.path.join(nc_folder, file), 'r')\n",
    "\n",
    "            time_var = ds.variables['time']\n",
    "            times = num2date(time_var[:], units=time_var.units, only_use_cftime_datetimes=False)\n",
    "\n",
    "            ps = ds.variables['PS'][:, 0, 0]\n",
    "            qv2m = ds.variables['QV2M'][:, 0, 0]\n",
    "            t2m = ds.variables['T2M'][:, 0, 0]\n",
    "            ts = ds.variables['TS'][:, 0, 0]\n",
    "            u10m = ds.variables['U10M'][:, 0, 0]\n",
    "            # v10m = ds.variables['V10M'][:, 0, 0]\n",
    "            qv10m = ds.variables['QV10M'][:, 0, 0]\n",
    "            slp = ds.variables['SLP'][:, 0, 0]\n",
    "            t10m = ds.variables['T10M'][:, 0, 0]\n",
    "            t2mdew = ds.variables['T2MDEW'][:, 0, 0]\n",
    "            tqi = ds.variables['TQI'][:, 0, 0]\n",
    "            tql = ds.variables['TQL'][:, 0, 0]\n",
    "\n",
    "            for i in range(len(times)):\n",
    "                # Ensure times[i] is a native datetime object\n",
    "                date_val = times[i]\n",
    "                if hasattr(date_val, 'year'):\n",
    "                    date_val = datetime(date_val.year, date_val.month, date_val.day)\n",
    "\n",
    "                records.append({\n",
    "                    \"Date\": date_val,\n",
    "                    \"PS\": ps[i],\n",
    "                    \"QV2M\": qv2m[i],\n",
    "                    \"T2M\": t2m[i],\n",
    "                    \"TS\": ts[i],\n",
    "                    \"U10M\": u10m[i],\n",
    "                    \"QV10M\": qv10m[i],\n",
    "                    \"SLP\": slp[i],\n",
    "                    \"T10M\": t10m[i],\n",
    "                    \"T2MDEW\": t2mdew[i],\n",
    "                    \"TQI\": tqi[i],\n",
    "                    \"TQL\": tql[i],\n",
    "                    # \"U2M\": u2m[i]\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "merra_df = extract_merra_features(\"merra_downloads\")  # 📁 Folder containing .nc4 files\n",
    "merra_df = merra_df.groupby(\"Date\").mean().reset_index()\n",
    "\n",
    "aod_df['Date'] = pd.to_datetime(aod_df['Date'])\n",
    "pm_df['Date'] = pd.to_datetime(pm_df['Date'])\n",
    "merra_df['Date'] = pd.to_datetime(merra_df['Date'])  # ✅ This fixes the issue\n",
    "\n",
    "# print(aod_df.dtypes)\n",
    "# print(pm_df.dtypes)\n",
    "# print(merra_df.dtypes)\n",
    "\n",
    "# print(f\"AOD records: {len(aod_df)}\")\n",
    "# print(f\"CPCB records: {len(pm_df)}\")\n",
    "# print(f\"MERRA records: {len(merra_df)}\")\n",
    "\n",
    "# ---------- Step 4: Merge All ----------\n",
    "combined_df = pd.merge(aod_df, pm_df, on=\"Date\")\n",
    "# print(f\"After merging AOD + CPCB: {len(combined_df)} records\")\n",
    "\n",
    "combined_df = pd.merge(combined_df, merra_df, on=\"Date\")\n",
    "# print(f\"After merging with MERRA: {len(combined_df)} records\")\n",
    "\n",
    "# print(\"🔍 AOD dates:\", aod_df['Date'].unique())\n",
    "# print(\"🔍 CPCB dates:\", pm_df['Date'].unique())\n",
    "# print(\"🔍 MERRA dates:\", merra_df['Date'].unique())\n",
    "\n",
    "\n",
    "# ---------- Step 5: ML Model ----------\n",
    "features = ['Mean_AOD', 'PS', 'QV2M', 'T2M', 'TS', 'U10M', 'QV10M', 'SLP', 'T10M', 'T2MDEW', 'TQI', 'TQL']\n",
    "clean_df = combined_df.dropna(subset=features + ['PM2.5 (µg/m³)'])\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "X = clean_df[features].copy()\n",
    "y = clean_df['PM2.5 (µg/m³)']\n",
    "\n",
    "# Optional feature engineering\n",
    "X['Temp_Diff'] = X['TS'] - X['T2M']\n",
    "X['Humidity_Ratio'] = X['QV2M'] / (X['T2M'] + 1e-3)  # prevent division by zero\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"✅ Evaluation:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# # ---------- Step 6: Predict for 20 June ----------\n",
    "# # Replace with actual June 20 values from your MERRA and AOD\n",
    "may_11 = {\n",
    "    \"Mean_AOD\": [0.97],\n",
    "    \"PS\": [98119.390],\n",
    "    \"QV2M\": [0.007],\n",
    "    \"T2M\": [313.68],\n",
    "    \"TS\": [316.82],\n",
    "    \"U10M\": [2.75],\n",
    "    \"QV10M\": [0.007],\n",
    "    \"SLP\": [100396.60],\n",
    "    \"T10M\": [312.85],\n",
    "    \"T2MDEW\": [283.28],\n",
    "    \"TQI\": [0.0],\n",
    "    \"TQL\": [0.0],\n",
    "}\n",
    "may_11_df = pd.DataFrame(may_11)\n",
    "\n",
    "# Recreate engineered features\n",
    "may_11_df['Temp_Diff'] = may_11_df['TS'] - may_11_df['T2M']\n",
    "may_11_df['Humidity_Ratio'] = may_11_df['QV2M'] / (may_11_df['T2M'] + 1e-3)\n",
    "\n",
    "# Ensure columns match training\n",
    "X_input = may_11_df[X_train.columns]\n",
    "\n",
    "# Predict\n",
    "pred_pm = rf.predict(X_input)\n",
    "print(\"\\n🔮 Predicted PM2.5 for 11 May 2024 at 05:30 IST:\", pred_pm[0])\n",
    "\n",
    "# ---------- Step 7: Save Prediction to CSV ----------\n",
    "output_df = pd.DataFrame({\n",
    "    'Date': ['2024-05-11'],\n",
    "    'Predicted_PM2.5': [pred_pm[0]]\n",
    "})\n",
    "\n",
    "# Save to CSV (overwrite every time)\n",
    "output_df.to_csv('predicted_pm25.csv', index=False, mode='w')\n",
    "\n",
    "print(\"💾 Prediction saved to 'predicted_pm25.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74f151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Keys in file: ['AOD', 'latitude', 'longitude', 'time']\n",
      "📐 AOD shape: (1, 551, 551)\n",
      "🧭 Latitude shape: (551,)\n",
      "🧭 Longitude shape: (551,)\n",
      "🔎 Nearest index — lat: 166 lon: 322\n",
      "📌 AOD value at Faridabad: 0.779472\n",
      "📆 Date parsed from filename: 2024-05-01\n",
      "✅ Final Record: {'Date': datetime.date(2024, 5, 1), 'Mean_AOD': 0.779472}\n"
     ]
    }
   ],
   "source": [
    "#To read AOD value from a single .h5 file\n",
    "import h5py\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# 📍 Location of interest: Faridabad (approx.)\n",
    "target_lat = 28.4\n",
    "target_lon = 77.3\n",
    "\n",
    "# 📄 Path to one .h5 file (example)\n",
    "file_path = \"aod_folder/3DIMG_01MAY2024_0530_L2G_AOD_V02R00.h5\"\n",
    "\n",
    "# ✅ Read and extract AOD value\n",
    "with h5py.File(file_path, 'r') as f:\n",
    "    print(\"🔍 Keys in file:\", list(f.keys()))  # Show root-level keys\n",
    "\n",
    "    # Check if required datasets are present\n",
    "    if 'AOD' in f and 'latitude' in f and 'longitude' in f:\n",
    "        aod_data = f['AOD'][:]           # 2D array of AOD\n",
    "        latitudes = f['latitude'][:]     # 1D array\n",
    "        longitudes = f['longitude'][:]   # 1D array\n",
    "\n",
    "        print(\"📐 AOD shape:\", aod_data.shape)\n",
    "        print(\"🧭 Latitude shape:\", latitudes.shape)\n",
    "        print(\"🧭 Longitude shape:\", longitudes.shape)\n",
    "\n",
    "        # Find nearest grid index to target lat/lon\n",
    "        lat_idx = (np.abs(latitudes - target_lat)).argmin()\n",
    "        lon_idx = (np.abs(longitudes - target_lon)).argmin()\n",
    "        print(\"🔎 Nearest index — lat:\", lat_idx, \"lon:\", lon_idx)\n",
    "\n",
    "        # Handle possible shape issues\n",
    "        try:\n",
    "            aod_value = aod_data[0, lat_idx, lon_idx]\n",
    "            print(\"📌 AOD value at Faridabad:\", aod_value)\n",
    "        except IndexError as e:\n",
    "            print(\"❌ IndexError while accessing AOD value:\", e)\n",
    "            aod_value = np.nan\n",
    "    else:\n",
    "        print(\"❌ One or more required datasets not found in file.\")\n",
    "        aod_value = np.nan\n",
    "\n",
    "    # Parse date from filename\n",
    "    filename = os.path.basename(file_path)\n",
    "    try:\n",
    "        date_str = filename.split(\"_\")[1]  # '01JUN2024'\n",
    "        date_obj = datetime.strptime(date_str, \"%d%b%Y\").date()\n",
    "        print(\"📆 Date parsed from filename:\", date_obj)\n",
    "    except Exception as e:\n",
    "        print(\"❌ Error parsing date from filename:\", e)\n",
    "        date_obj = None\n",
    "\n",
    "    # Store result\n",
    "    record = {\n",
    "        \"Date\": date_obj,\n",
    "        \"Mean_AOD\": aod_value\n",
    "    }\n",
    "\n",
    "print(\"✅ Final Record:\", record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7935251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved all AOD values to aod_data.csv\n"
     ]
    }
   ],
   "source": [
    "#Merging all the .h5 into a single csv file\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def extract_aod_from_folder(folder_path):\n",
    "    records = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".h5\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                with h5py.File(file_path, 'r') as f:\n",
    "                    # Ensure keys exist (some files might be corrupt)\n",
    "                    if 'AOD' not in f or 'latitude' not in f or 'longitude' not in f:\n",
    "                        continue\n",
    "\n",
    "                    aod_data = f['AOD'][:]\n",
    "                    latitudes = f['latitude'][:]\n",
    "                    longitudes = f['longitude'][:]\n",
    "\n",
    "                    # Find nearest point to Faridabad\n",
    "                    lat_idx = (np.abs(latitudes - 28.4)).argmin()\n",
    "                    lon_idx = (np.abs(longitudes - 77.3)).argmin()\n",
    "\n",
    "                    # Fix for shape mismatch\n",
    "                    if aod_data.ndim == 3 and lat_idx < aod_data.shape[1] and lon_idx < aod_data.shape[2]:\n",
    "                        aod_val = aod_data[0, lat_idx, lon_idx]\n",
    "                    else:\n",
    "                        aod_val = np.nan  # skip if shape mismatch\n",
    "\n",
    "                    # Extract date from filename\n",
    "                    date_str = filename.split(\"_\")[1]  # '01JUN2024'\n",
    "                    date_obj = datetime.strptime(date_str, \"%d%b%Y\").date()\n",
    "\n",
    "                    records.append({\n",
    "                        \"Date\": date_obj,\n",
    "                        \"Mean_AOD\": aod_val\n",
    "                    })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed for {filename}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# 🔁 Extract from all files\n",
    "aod_df = extract_aod_from_folder(\"aod_folder\")\n",
    "\n",
    "# 💾 Save to CSV\n",
    "aod_df.to_csv(\"aod_data.csv\", index=False)\n",
    "print(\"✅ Saved all AOD values to aod_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c1197b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PS': [98119.390625], 'QV2M': [0.007875049486756325], 'T2M': [313.6807861328125], 'TS': [316.8280334472656], 'U10M': [2.7565226554870605], 'QV10M': [0.007863740436732769], 'SLP': [100396.609375], 'T10M': [312.8543701171875], 'T2MDEW': [283.28582763671875], 'TQI': [0.0], 'TQL': [0.0]}\n"
     ]
    }
   ],
   "source": [
    "#Reading the MERRA Varible values from the file\n",
    "import xarray as xr\n",
    "\n",
    "# Load NetCDF file\n",
    "file_path = \"merra_downloads\\MERRA2_400.tavg1_2d_slv_Nx.20240511.SUB.nc\"\n",
    "ds = xr.open_dataset(file_path)\n",
    "\n",
    "# 🔍 Step 1: Define coordinates of Faridabad (approx)\n",
    "target_lat = 28.41\n",
    "target_lon = 77.31\n",
    "\n",
    "# 🔍 Step 2: Select the nearest location\n",
    "nearest_lat = ds.sel(lat=target_lat, method=\"nearest\").lat.values\n",
    "nearest_lon = ds.sel(lon=target_lon, method=\"nearest\").lon.values\n",
    "\n",
    "# 🔍 Step 3: Select the first time step (if only one day is present)\n",
    "time_step = ds.time.values[0]\n",
    "\n",
    "# 🔍 Step 4: Extract values for each variable at that location and time\n",
    "may_11 = {\n",
    "    \"PS\":     [float(ds[\"PS\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"QV2M\":   [float(ds[\"QV2M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"T2M\":    [float(ds[\"T2M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"TS\":     [float(ds[\"TS\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"U10M\":   [float(ds[\"U10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"QV10M\":   [float(ds[\"QV10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"SLP\":   [float(ds[\"SLP\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"T10M\":   [float(ds[\"T10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"T2MDEW\":   [float(ds[\"T2MDEW\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"TQI\":   [float(ds[\"TQI\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    \"TQL\":   [float(ds[\"TQL\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    # \"U2M\":   [float(ds[\"U2M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    # \"Mean_AOD\": [0.97]  # Add manually or fetch from INSAT/CPCB if needed\n",
    "}\n",
    "\n",
    "print(may_11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd706a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation:\n",
      "MAE: 28.026991176470588\n",
      "RMSE: 36.08490578960144\n",
      "R²: 0.0041481018634580424\n",
      "\n",
      "🔮 Predicted PM2.5 for 03-04-2024 around 05:30 IST: 105.40529999999995\n",
      "💾 Prediction saved to 'predicted_pm25.csv'\n"
     ]
    }
   ],
   "source": [
    "#Combining MOSDAC and CPCB data with MERRA and predicting the PM2.5 Level(SAutomatic filling of MERRA DATA)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from netCDF4 import Dataset, num2date\n",
    "from datetime import datetime\n",
    "import os\n",
    "import xarray as xr\n",
    "\n",
    "# ---------- Step 1: Load INSAT AOD from CSV ----------\n",
    "aod_df = pd.read_csv(\"aod_data.csv\")\n",
    "\n",
    "# Parse with mixed formats; keep rows that fail as NaT so you can inspect\n",
    "aod_df['Date'] = pd.to_datetime(\n",
    "    aod_df['Date'], \n",
    "    format='mixed',     # pandas >= 2.0\n",
    "    dayfirst=True, \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "bad_aod = aod_df[aod_df['Date'].isna()]\n",
    "if not bad_aod.empty:\n",
    "    print(\"Unparsable AOD dates (fix in CSV):\")\n",
    "    print(bad_aod.head(10))\n",
    "\n",
    "# ---------- Step 2: Load CPCB PM2.5 ----------\n",
    "pm_df = pd.read_csv(\"combined_pm2.5_para.csv\")\n",
    "\n",
    "pm_df['Timestamp'] = pd.to_datetime(\n",
    "    pm_df['Timestamp'], \n",
    "    format='mixed', \n",
    "    dayfirst=True, \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "bad_pm = pm_df[pm_df['Timestamp'].isna()]\n",
    "if not bad_pm.empty:\n",
    "    print(\"Unparsable CPCB timestamps (fix in CSV):\")\n",
    "    print(bad_pm[['Timestamp']].head(10))\n",
    "\n",
    "pm_df['Date'] = pm_df['Timestamp'].dt.normalize()\n",
    "\n",
    "# ---------- Step 3: Load MERRA .nc4 files ----------\n",
    "def extract_merra_features(nc_folder):\n",
    "    records = []\n",
    "    for file in os.listdir(nc_folder):\n",
    "        if file.endswith(\".nc\"):\n",
    "            ds = Dataset(os.path.join(nc_folder, file), 'r')\n",
    "\n",
    "            time_var = ds.variables['time']\n",
    "            times = num2date(time_var[:], units=time_var.units, only_use_cftime_datetimes=False)\n",
    "\n",
    "            ps = ds.variables['PS'][:, 0, 0]\n",
    "            qv2m = ds.variables['QV2M'][:, 0, 0]\n",
    "            t2m = ds.variables['T2M'][:, 0, 0]\n",
    "            ts = ds.variables['TS'][:, 0, 0]\n",
    "            u10m = ds.variables['U10M'][:, 0, 0]\n",
    "            # v10m = ds.variables['V10M'][:, 0, 0]\n",
    "            qv10m = ds.variables['QV10M'][:, 0, 0]\n",
    "            slp = ds.variables['SLP'][:, 0, 0]\n",
    "            t10m = ds.variables['T10M'][:, 0, 0]\n",
    "            t2mdew = ds.variables['T2MDEW'][:, 0, 0]\n",
    "            tqi = ds.variables['TQI'][:, 0, 0]\n",
    "            tql = ds.variables['TQL'][:, 0, 0]\n",
    "\n",
    "            for i in range(len(times)):\n",
    "                # Ensure times[i] is a native datetime object\n",
    "                date_val = times[i]\n",
    "                if hasattr(date_val, 'year'):\n",
    "                    date_val = datetime(date_val.year, date_val.month, date_val.day)\n",
    "\n",
    "                records.append({\n",
    "                    \"Date\": date_val,\n",
    "                    \"PS\": ps[i],\n",
    "                    \"QV2M\": qv2m[i],\n",
    "                    \"T2M\": t2m[i],\n",
    "                    \"TS\": ts[i],\n",
    "                    \"U10M\": u10m[i],\n",
    "                    \"QV10M\": qv10m[i],\n",
    "                    \"SLP\": slp[i],\n",
    "                    \"T10M\": t10m[i],\n",
    "                    \"T2MDEW\": t2mdew[i],\n",
    "                    \"TQI\": tqi[i],\n",
    "                    \"TQL\": tql[i],\n",
    "                    # \"U2M\": u2m[i]\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "merra_df = extract_merra_features(\"merra_downloads\")  \n",
    "merra_df = merra_df.groupby(\"Date\").mean().reset_index()\n",
    "\n",
    "aod_df['Date'] = pd.to_datetime(aod_df['Date'])\n",
    "pm_df['Date'] = pd.to_datetime(pm_df['Date'])\n",
    "merra_df['Date'] = pd.to_datetime(merra_df['Date'])  \n",
    "\n",
    "# print(aod_df.dtypes)\n",
    "# print(pm_df.dtypes)\n",
    "# print(merra_df.dtypes)\n",
    "\n",
    "# print(f\"AOD records: {len(aod_df)}\")\n",
    "# print(f\"CPCB records: {len(pm_df)}\")\n",
    "# print(f\"MERRA records: {len(merra_df)}\")\n",
    "\n",
    "# ---------- Step 4: Merge All ----------\n",
    "combined_df = pd.merge(aod_df, pm_df, on=\"Date\")\n",
    "# print(f\"After merging AOD + CPCB: {len(combined_df)} records\")\n",
    "\n",
    "combined_df = pd.merge(combined_df, merra_df, on=\"Date\")\n",
    "# print(f\"After merging with MERRA: {len(combined_df)} records\")\n",
    "\n",
    "# print(\"🔍 AOD dates:\", aod_df['Date'].unique())\n",
    "# print(\"🔍 CPCB dates:\", pm_df['Date'].unique())\n",
    "# print(\"🔍 MERRA dates:\", merra_df['Date'].unique())\n",
    "\n",
    "\n",
    "# ---------- Step 5: ML Model ----------\n",
    "features = ['Mean_AOD', 'PS', 'QV2M', 'T2M', 'TS', 'U10M', 'QV10M', 'SLP', 'T10M', 'T2MDEW', 'TQI', 'TQL']\n",
    "clean_df = combined_df.dropna(subset=features + ['PM2.5 (µg/m³)'])\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "X = clean_df[features].copy()\n",
    "y = clean_df['PM2.5 (µg/m³)']\n",
    "\n",
    "# Optional feature engineering\n",
    "X['Temp_Diff'] = X['TS'] - X['T2M']\n",
    "X['Humidity_Ratio'] = X['QV2M'] / (X['T2M'] + 1e-3)  # prevent division by zero\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"✅ Evaluation:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# ---------- Step 6: Extract MERRA features for a specific date ----------\n",
    "def extract_merra_single_day(file_path, lat=28.41, lon=77.31):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    nearest_lat = ds.sel(lat=lat, method=\"nearest\").lat.values\n",
    "    nearest_lon = ds.sel(lon=lon, method=\"nearest\").lon.values\n",
    "\n",
    "    time_step = ds.time.values[0]  # Assuming only one day present\n",
    "\n",
    "    features = {\n",
    "        \"PS\":     [float(ds[\"PS\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"QV2M\":   [float(ds[\"QV2M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"T2M\":    [float(ds[\"T2M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"TS\":     [float(ds[\"TS\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"U10M\":   [float(ds[\"U10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"QV10M\":  [float(ds[\"QV10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"SLP\":    [float(ds[\"SLP\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"T10M\":   [float(ds[\"T10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"T2MDEW\": [float(ds[\"T2MDEW\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"TQI\":    [float(ds[\"TQI\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"TQL\":    [float(ds[\"TQL\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    }\n",
    "\n",
    "    return features\n",
    "\n",
    "# Update this path to the correct file for the target date\n",
    "merra_file = \"merra_downloads/MERRA2_400.tavg1_2d_slv_Nx.20240403.SUB.nc\"\n",
    "merra_features = extract_merra_single_day(merra_file)\n",
    "\n",
    "# Add manually or automate fetching AOD from INSAT later\n",
    "merra_features[\"Mean_AOD\"] = [0.97]\n",
    "\n",
    "# Create DataFrame from extracted features\n",
    "predict_input_df = pd.DataFrame(merra_features)\n",
    "\n",
    "# ---------- Step 7: Feature Engineering ----------\n",
    "predict_input_df['Temp_Diff'] = predict_input_df['TS'] - predict_input_df['T2M']\n",
    "predict_input_df['Humidity_Ratio'] = predict_input_df['QV2M'] / (predict_input_df['T2M'] + 1e-3)\n",
    "\n",
    "# Ensure correct column order\n",
    "X_input = predict_input_df[X_train.columns]\n",
    "\n",
    "# ---------- Step 8: Predict ----------\n",
    "pred_pm = rf.predict(X_input)\n",
    "print(\"\\n🔮 Predicted PM2.5 for 03-04-2024 around 05:30 IST:\", pred_pm[0])\n",
    "\n",
    "# ---------- Step 9: Save prediction ----------\n",
    "output_df = pd.DataFrame({\n",
    "    'Date': ['2024-05-11'],\n",
    "    'Predicted_PM2.5': [pred_pm[0]]\n",
    "})\n",
    "output_df.to_csv('predicted_pm25.csv', index=False, mode='w')\n",
    "print(\"💾 Prediction saved to 'predicted_pm25.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a789681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Timestamp  PM2.5 (µg/m³)\n",
      "2237 2024-04-03 05:00:00         176.48\n"
     ]
    }
   ],
   "source": [
    "#Finding PM2.5 concentration for a single day\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"CPCB Data/City_wise_raw_data_1Hr_2024_Faridabad_1Hr.csv\")\n",
    "df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], errors = \"coerce\")\n",
    "target_time = pd.Timestamp(\"2024-04-03 05:00:00\")\n",
    "pm_at_time = df[df[\"Timestamp\"] == target_time]\n",
    "print(pm_at_time[[\"Timestamp\", \"PM2.5 (µg/m³)\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4fe50548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation Metrics:\n",
      "MAE : 153.92415000000034\n",
      "RMSE: 160.2998252595538\n",
      "R²  : -13.265200219749412\n",
      "\n",
      "🔮 Predicted PM2.5 for 2024-05-11 IST: 122.24639999999918\n",
      "💾 Prediction saved to 'predicted_pm25.csv'\n"
     ]
    }
   ],
   "source": [
    "#Improved version of above acc to GPT-5\n",
    "# -------------------- IMPORTS --------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xarray as xr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "# -------------------- CONSTANTS --------------------\n",
    "CITY_LAT, CITY_LON = 28.41, 77.31   # Faridabad location\n",
    "IST = \"Asia/Kolkata\"\n",
    "MERRA_VARS = [\"PS\",\"QV2M\",\"T2M\",\"TS\",\"U10M\",\"QV10M\",\"SLP\",\"T10M\",\"T2MDEW\",\"TQI\",\"TQL\"]  # Add V10M if available\n",
    "\n",
    "# -------------------- STEP 1: LOAD AOD --------------------\n",
    "aod_df = pd.read_csv(\"aod_data.csv\")\n",
    "aod_df[\"Date\"] = pd.to_datetime(aod_df[\"Date\"])\n",
    "aod_df[\"Date\"] = aod_df[\"Date\"].dt.tz_localize(IST, nonexistent=\"shift_forward\", ambiguous=\"NaT\").dt.floor(\"D\").dt.tz_localize(None)\n",
    "\n",
    "# -------------------- STEP 2: LOAD CPCB PM2.5 --------------------\n",
    "pm_df = pd.read_csv(\"combined_pm2.5_para.csv\")\n",
    "pm_df[\"Timestamp\"] = pd.to_datetime(pm_df[\"Timestamp\"])\n",
    "pm_df[\"Timestamp\"] = pm_df[\"Timestamp\"].dt.tz_localize(IST, nonexistent=\"shift_forward\", ambiguous=\"NaT\")\n",
    "pm_df[\"Date\"] = pm_df[\"Timestamp\"].dt.floor(\"D\").dt.tz_localize(None)\n",
    "\n",
    "# Keep only date + PM columns\n",
    "pm_df = pm_df[[\"Date\", \"PM2.5 (µg/m³)\"]]\n",
    "\n",
    "# -------------------- STEP 3: LOAD MERRA DATA --------------------\n",
    "def extract_merra_features_xr(nc_folder, lat=CITY_LAT, lon=CITY_LON):\n",
    "    frames = []\n",
    "    for f in os.listdir(nc_folder):\n",
    "        if f.endswith((\".nc\", \".nc4\")):\n",
    "            ds = xr.open_dataset(os.path.join(nc_folder, f))\n",
    "            sub = ds.sel(lat=lat, lon=lon, method=\"nearest\")[MERRA_VARS]\n",
    "            df = sub.to_dataframe().reset_index()\n",
    "            df[\"Date\"] = (pd.to_datetime(df[\"time\"])\n",
    "                          .dt.tz_localize(\"UTC\")\n",
    "                          .dt.tz_convert(IST)\n",
    "                          .dt.floor(\"D\")\n",
    "                          .dt.tz_localize(None))\n",
    "            df = df.groupby(\"Date\")[MERRA_VARS].mean().reset_index()\n",
    "            frames.append(df)\n",
    "    merra = pd.concat(frames, ignore_index=True)\n",
    "    merra = merra.groupby(\"Date\", as_index=False).mean()\n",
    "    return merra\n",
    "\n",
    "merra_df = extract_merra_features_xr(\"merra_downloads\")\n",
    "\n",
    "# -------------------- STEP 4: MERGE ALL --------------------\n",
    "combined_df = (aod_df.merge(pm_df, on=\"Date\", how=\"inner\")\n",
    "                        .merge(merra_df, on=\"Date\", how=\"inner\"))\n",
    "\n",
    "# -------------------- STEP 5: CLEAN & FEATURE ENGINEERING --------------------\n",
    "combined_df[\"PM2.5 (µg/m³)\"] = pd.to_numeric(combined_df[\"PM2.5 (µg/m³)\"], errors=\"coerce\")\n",
    "\n",
    "features = [\"Mean_AOD\"] + MERRA_VARS\n",
    "clean_df = combined_df.dropna(subset=features + [\"PM2.5 (µg/m³)\"]).copy()\n",
    "\n",
    "clean_df[\"Temp_Diff\"] = clean_df[\"TS\"] - clean_df[\"T2M\"]\n",
    "clean_df[\"Humidity_Ratio\"] = clean_df[\"QV2M\"] / (clean_df[\"T2M\"] + 1e-3)\n",
    "\n",
    "X_cols = features + [\"Temp_Diff\", \"Humidity_Ratio\"]\n",
    "\n",
    "# -------------------- STEP 6: TIME-AWARE TRAIN/TEST SPLIT --------------------\n",
    "clean_df = clean_df.sort_values(\"Date\")\n",
    "split_idx = int(len(clean_df) * 0.8)\n",
    "train_df = clean_df.iloc[:split_idx]\n",
    "test_df = clean_df.iloc[split_idx:]\n",
    "\n",
    "X_train, y_train = train_df[X_cols], train_df[\"PM2.5 (µg/m³)\"]\n",
    "X_test, y_test = test_df[X_cols], test_df[\"PM2.5 (µg/m³)\"]\n",
    "\n",
    "# -------------------- STEP 7: TRAIN MODEL --------------------\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# -------------------- STEP 8: EVALUATE --------------------\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"✅ Evaluation Metrics:\")\n",
    "print(\"MAE :\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"R²  :\", r2_score(y_test, y_pred))\n",
    "\n",
    "# -------------------- STEP 9: PREDICT FOR A NEW DAY --------------------\n",
    "def extract_merra_day_mean(file_path, date_ist, lat=CITY_LAT, lon=CITY_LON):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    sub = ds.sel(lat=lat, lon=lon, method=\"nearest\")[MERRA_VARS]\n",
    "    df = sub.to_dataframe().reset_index()\n",
    "    df[\"Date\"] = (pd.to_datetime(df[\"time\"])\n",
    "                  .dt.tz_localize(\"UTC\")\n",
    "                  .dt.tz_convert(IST)\n",
    "                  .dt.floor(\"D\")\n",
    "                  .dt.tz_localize(None))\n",
    "    day = pd.to_datetime(date_ist)\n",
    "    day_df = df[df[\"Date\"] == day]\n",
    "    feats = day_df[MERRA_VARS].mean().to_dict()\n",
    "    return feats\n",
    "\n",
    "# Example date: 2024-05-10\n",
    "merra_file = \"merra_downloads/MERRA2_400.tavg1_2d_slv_Nx.20240511.SUB.nc\"\n",
    "merra_features = extract_merra_day_mean(merra_file, \"2024-05-11\")\n",
    "merra_features[\"Mean_AOD\"] = 0.97  # Replace with real AOD if available\n",
    "\n",
    "predict_input_df = pd.DataFrame([merra_features])\n",
    "predict_input_df[\"Temp_Diff\"] = predict_input_df[\"TS\"] - predict_input_df[\"T2M\"]\n",
    "predict_input_df[\"Humidity_Ratio\"] = predict_input_df[\"QV2M\"] / (predict_input_df[\"T2M\"] + 1e-3)\n",
    "\n",
    "pred_pm = rf.predict(predict_input_df[X_cols])[0]\n",
    "print(f\"\\n🔮 Predicted PM2.5 for 2024-05-11 IST: {pred_pm}\")\n",
    "\n",
    "# -------------------- STEP 10: SAVE PREDICTION --------------------\n",
    "output_df = pd.DataFrame({\n",
    "    \"Date\": [\"2024-05-10\"],\n",
    "    \"Predicted_PM2.5\": [pred_pm]\n",
    "})\n",
    "output_df.to_csv(\"predicted_pm25.csv\", index=False)\n",
    "print(\"💾 Prediction saved to 'predicted_pm25.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee6a3621",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "['Mean_AOD']",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_18312\\1617395456.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    215\u001b[39m combined_df = pd.merge(combined_df, merra_df, on=\u001b[33m\"Date\"\u001b[39m, how=\u001b[33m\"outer\"\u001b[39m)\n\u001b[32m    216\u001b[39m \n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# ------------------- ML & CLUSTERING -------------------\u001b[39;00m\n\u001b[32m    218\u001b[39m features = [\u001b[33m'Mean_AOD'\u001b[39m, \u001b[33m'PS'\u001b[39m, \u001b[33m'QV2M'\u001b[39m, \u001b[33m'T2M'\u001b[39m, \u001b[33m'TS'\u001b[39m, \u001b[33m'U10M'\u001b[39m, \u001b[33m'QV10M'\u001b[39m, \u001b[33m'SLP'\u001b[39m, \u001b[33m'T10M'\u001b[39m, \u001b[33m'T2MDEW'\u001b[39m, \u001b[33m'TQI'\u001b[39m, \u001b[33m'TQL'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m clean_df = combined_df.dropna(subset=features + [\u001b[33m'PM2.5 (µg/m³)'\u001b[39m]).copy()\n\u001b[32m    220\u001b[39m clean_df[\u001b[33m'Temp_Diff'\u001b[39m] = clean_df[\u001b[33m'TS'\u001b[39m] - clean_df[\u001b[33m'T2M'\u001b[39m]\n\u001b[32m    221\u001b[39m clean_df[\u001b[33m'Humidity_Ratio'\u001b[39m] = clean_df[\u001b[33m'QV2M'\u001b[39m] / (clean_df[\u001b[33m'T2M'\u001b[39m] + \u001b[32m1e-3\u001b[39m)\n\u001b[32m    222\u001b[39m \n",
      "\u001b[32mc:\\Users\\91931\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[39m\n\u001b[32m   6666\u001b[39m             ax = self._get_axis(agg_axis)\n\u001b[32m   6667\u001b[39m             indices = ax.get_indexer_for(subset)\n\u001b[32m   6668\u001b[39m             check = indices == -\u001b[32m1\u001b[39m\n\u001b[32m   6669\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m check.any():\n\u001b[32m-> \u001b[39m\u001b[32m6670\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m KeyError(np.array(subset)[check].tolist())\n\u001b[32m   6671\u001b[39m             agg_obj = self.take(indices, axis=agg_axis)\n\u001b[32m   6672\u001b[39m \n\u001b[32m   6673\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m thresh \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m lib.no_default:\n",
      "\u001b[31mKeyError\u001b[39m: ['Mean_AOD']"
     ]
    }
   ],
   "source": [
    "#Code including the approach of clustering as well as regression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "from netCDF4 import Dataset, num2date\n",
    "from datetime import datetime\n",
    "import os\n",
    "import xarray as xr\n",
    "import re\n",
    "\n",
    "def extract_aod_from_nc(nc_folder, lat=28.41, lon=77.31):\n",
    "    \"\"\"\n",
    "    Read .nc files in `nc_folder`, detect coordinate/var names automatically,\n",
    "    extract AOD at nearest gridpoint to (lat, lon) for each time step or snapshot.\n",
    "    Returns DataFrame with columns: Date, Mean_AOD_nc\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for fname in os.listdir(nc_folder):\n",
    "        if not fname.endswith(\".nc\"):\n",
    "            continue\n",
    "        path = os.path.join(nc_folder, fname)\n",
    "        ds = xr.open_dataset(path, decode_times=False)  # decode_times False initially\n",
    "\n",
    "        # --- detect time coordinate name ---\n",
    "        time_candidates = ['time', 'Time', 'TIME', 'times', 'date', 'Date', 'datetime', 'DATETIME', 't', 'TIME_UTC']\n",
    "        time_name = next((c for c in time_candidates if c in ds.coords or c in ds.variables), None)\n",
    "\n",
    "        if time_name is None:\n",
    "            # try automatic detection by dtype (variables that are datetime64)\n",
    "            for k in ds.variables:\n",
    "                try:\n",
    "                    if np.issubdtype(ds[k].values.dtype, np.datetime64):\n",
    "                        time_name = k\n",
    "                        break\n",
    "                except Exception:\n",
    "                    pass\n",
    "        if time_name is None:\n",
    "            # try xarray's decoded time via decode_cf\n",
    "            try:\n",
    "                ds2 = xr.open_dataset(path, decode_times=True)\n",
    "                time_name = next((c for c in ds2.coords if np.issubdtype(ds2[c].values.dtype, np.datetime64)), None)\n",
    "                if time_name:\n",
    "                    ds = ds2\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # ----------- UPDATED SECTION: handle no time coordinate --------------\n",
    "        if time_name is None:\n",
    "            match = re.search(r'(\\d{8})', fname)\n",
    "            if match:\n",
    "                date_val = pd.to_datetime(match.group(1), format='%Y%m%d')\n",
    "            else:\n",
    "                date_val = pd.Timestamp.today().normalize()\n",
    "                print(f\"⚠️ Could not find date in filename {fname}, using today's date.\")\n",
    "\n",
    "            lat_candidates = ['lat', 'latitude', 'LAT', 'Latitude']\n",
    "            lon_candidates = ['lon', 'longitude', 'LON', 'Longitude']\n",
    "            lat_name = next((c for c in lat_candidates if c in ds.coords or c in ds.variables), None)\n",
    "            lon_name = next((c for c in lon_candidates if c in ds.coords or c in ds.variables), None)\n",
    "            if lat_name is None or lon_name is None:\n",
    "                raise ValueError(f\"No lat/lon coords found in {fname}. coords: {list(ds.coords.keys())}\")\n",
    "            aod_var = next((v for v in ds.data_vars if 'aod' in v.lower()), None)\n",
    "            if aod_var is None:\n",
    "                raise ValueError(f\"No AOD-like variable found in {fname}. data_vars: {list(ds.data_vars)}\")\n",
    "            lat_vals = ds[lat_name].values\n",
    "            lon_vals = ds[lon_name].values\n",
    "            aod_data = ds[aod_var].values\n",
    "\n",
    "            if getattr(lat_vals, 'ndim', 1) == 1 and getattr(lon_vals, 'ndim', 1) == 1:\n",
    "                nearest_lat = ds[lat_name].sel({lat_name: lat}, method=\"nearest\").values\n",
    "                nearest_lon = ds[lon_name].sel({lon_name: lon}, method=\"nearest\").values\n",
    "                aod_value = float(ds[aod_var].sel({lat_name: nearest_lat, lon_name: nearest_lon}).values)\n",
    "                records.append({\"Date\": date_val, \"Mean_AOD_nc\": aod_value})\n",
    "            else:\n",
    "                flat_lat = lat_vals.ravel()\n",
    "                flat_lon = lon_vals.ravel()\n",
    "                dist = (flat_lat - lat)**2 + (flat_lon - lon)**2\n",
    "                flat_idx = np.argmin(dist)\n",
    "                ij = np.unravel_index(flat_idx, lat_vals.shape)\n",
    "                aod_value = float(aod_data[ij])\n",
    "                records.append({\"Date\": date_val, \"Mean_AOD_nc\": aod_value})\n",
    "            continue\n",
    "\n",
    "        # --- detect lat/lon names ---\n",
    "        lat_candidates = ['lat', 'latitude', 'LAT', 'Latitude']\n",
    "        lon_candidates = ['lon', 'longitude', 'LON', 'Longitude']\n",
    "        lat_name = next((c for c in lat_candidates if c in ds.coords or c in ds.variables), None)\n",
    "        lon_name = next((c for c in lon_candidates if c in ds.coords or c in ds.variables), None)\n",
    "        if lat_name is None or lon_name is None:\n",
    "            raise ValueError(f\"No lat/lon coords found in {fname}. coords: {list(ds.coords.keys())}\")\n",
    "        aod_var = next((v for v in ds.data_vars if 'aod' in v.lower()), None)\n",
    "        if aod_var is None:\n",
    "            raise ValueError(f\"No AOD-like variable found in {fname}. data_vars: {list(ds.data_vars)}\")\n",
    "        lat_vals = ds[lat_name].values\n",
    "        lon_vals = ds[lon_name].values\n",
    "\n",
    "        if getattr(lat_vals, 'ndim', 1) == 1 and getattr(lon_vals, 'ndim', 1) == 1:\n",
    "            nearest_lat = ds[lat_name].sel({lat_name: lat}, method=\"nearest\").values\n",
    "            nearest_lon = ds[lon_name].sel({lon_name: lon}, method=\"nearest\").values\n",
    "            time_vals = ds[time_name].values\n",
    "            for t in time_vals:\n",
    "                try:\n",
    "                    date_val = pd.to_datetime(str(t)).normalize()\n",
    "                except Exception:\n",
    "                    date_val = pd.to_datetime(t).normalize()\n",
    "                aod_value = float(ds[aod_var].sel({time_name: t, lat_name: nearest_lat, lon_name: nearest_lon}).values)\n",
    "                records.append({\"Date\": date_val, \"Mean_AOD_nc\": aod_value})\n",
    "        else:\n",
    "            lat_arr = np.array(lat_vals)\n",
    "            lon_arr = np.array(lon_vals)\n",
    "            if lat_arr.shape != lon_arr.shape:\n",
    "                raise ValueError(f\"Lat/Lon shapes differ in {fname}: {lat_arr.shape} vs {lon_arr.shape}\")\n",
    "            flat_lat = lat_arr.ravel()\n",
    "            flat_lon = lon_arr.ravel()\n",
    "            dist = (flat_lat - lat)**2 + (flat_lon - lon)**2\n",
    "            flat_idx = np.argmin(dist)\n",
    "            ij = np.unravel_index(flat_idx, lat_arr.shape)\n",
    "            aod_da = ds[aod_var]\n",
    "            lat_dim = next((d for d in aod_da.dims if d in ds[lat_name].dims or 'lat' in d.lower()), None)\n",
    "            lon_dim = next((d for d in aod_da.dims if d in ds[lon_name].dims or 'lon' in d.lower()), None)\n",
    "            if lat_dim is None or lon_dim is None:\n",
    "                lat_dim = aod_da.dims[1]\n",
    "                lon_dim = aod_da.dims[2]\n",
    "            for t in ds[time_name].values:\n",
    "                try:\n",
    "                    date_val = pd.to_datetime(str(t)).normalize()\n",
    "                except Exception:\n",
    "                    date_val = pd.to_datetime(t).normalize()\n",
    "                aod_slice = aod_da.sel({time_name: t})\n",
    "                aod_value = float(aod_slice.isel({lat_dim: ij[0], lon_dim: ij[1]}).values)\n",
    "                records.append({\"Date\": date_val, \"Mean_AOD_nc\": aod_value})\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def extract_merra_features(nc_folder):\n",
    "    records = []\n",
    "    for file in os.listdir(nc_folder):\n",
    "        if file.endswith(\".nc\"):\n",
    "            ds = Dataset(os.path.join(nc_folder, file), 'r')\n",
    "            time_var = ds.variables['time']\n",
    "            times = num2date(time_var[:], units=time_var.units, only_use_cftime_datetimes=False)\n",
    "            ps = ds.variables['PS'][:, 0, 0]\n",
    "            qv2m = ds.variables['QV2M'][:, 0, 0]\n",
    "            t2m = ds.variables['T2M'][:, 0, 0]\n",
    "            ts = ds.variables['TS'][:, 0, 0]\n",
    "            u10m = ds.variables['U10M'][:, 0, 0]\n",
    "            qv10m = ds.variables['QV10M'][:, 0, 0]\n",
    "            slp = ds.variables['SLP'][:, 0, 0]\n",
    "            t10m = ds.variables['T10M'][:, 0, 0]\n",
    "            t2mdew = ds.variables['T2MDEW'][:, 0, 0]\n",
    "            tqi = ds.variables['TQI'][:, 0, 0]\n",
    "            tql = ds.variables['TQL'][:, 0, 0]\n",
    "            for i in range(len(times)):\n",
    "                date_val = times[i]\n",
    "                if hasattr(date_val, 'year'):\n",
    "                    date_val = datetime(date_val.year, date_val.month, date_val.day)\n",
    "                records.append({\n",
    "                    \"Date\": date_val,\n",
    "                    \"PS\": ps[i],\n",
    "                    \"QV2M\": qv2m[i],\n",
    "                    \"T2M\": t2m[i],\n",
    "                    \"TS\": ts[i],\n",
    "                    \"U10M\": u10m[i],\n",
    "                    \"QV10M\": qv10m[i],\n",
    "                    \"SLP\": slp[i],\n",
    "                    \"T10M\": t10m[i],\n",
    "                    \"T2MDEW\": t2mdew[i],\n",
    "                    \"TQI\": tqi[i],\n",
    "                    \"TQL\": tql[i],\n",
    "                })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def extract_merra_single_day(file_path, lat=28.41, lon=77.31):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    nearest_lat = ds.sel(lat=lat, method=\"nearest\").lat.values\n",
    "    nearest_lon = ds.sel(lon=lon, method=\"nearest\").lon.values\n",
    "    time_step = ds.time.values[0]  # Assuming only one day present\n",
    "    features = {\n",
    "        \"PS\":     [float(ds[\"PS\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"QV2M\":   [float(ds[\"QV2M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"T2M\":    [float(ds[\"T2M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"TS\":     [float(ds[\"TS\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"U10M\":   [float(ds[\"U10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"QV10M\":  [float(ds[\"QV10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"SLP\":    [float(ds[\"SLP\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"T10M\":   [float(ds[\"T10M\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"T2MDEW\": [float(ds[\"T2MDEW\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"TQI\":    [float(ds[\"TQI\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "        \"TQL\":    [float(ds[\"TQL\"].sel(time=time_step, lat=nearest_lat, lon=nearest_lon).values)],\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# -------------------- DATA LOADING --------------------\n",
    "\n",
    "aod_nc = extract_aod_from_nc(\"Latest_Data\")\n",
    "aod_nc['Date'] = pd.to_datetime(aod_nc['Date'])\n",
    "aod_nc.rename(columns={'Mean_AOD_nc': 'Mean_AOD'}, inplace=True)  # Ensure unified name\n",
    "\n",
    "aod_csv = pd.read_csv(\"aod_data.csv\")\n",
    "aod_csv['Date'] = pd.to_datetime(aod_csv['Date'])\n",
    "\n",
    "pm_df = pd.read_csv(\"pm25_cpcb_05AM_01janto18june.csv\")\n",
    "pm_df['Timestamp'] = pd.to_datetime(pm_df['Timestamp'])\n",
    "pm_df['Date'] = pm_df['Timestamp'].dt.normalize()\n",
    "\n",
    "merra_df = extract_merra_features(\"merra_downloads\")\n",
    "merra_df = merra_df.groupby(\"Date\").mean().reset_index()\n",
    "merra_df['Date'] = pd.to_datetime(merra_df['Date'])\n",
    "\n",
    "# Merge all on Date\n",
    "combined_df = pd.merge(aod_csv, pm_df, on=\"Date\", how=\"outer\")\n",
    "combined_df = pd.merge(combined_df, aod_nc, on=\"Date\", how=\"outer\")\n",
    "combined_df = pd.merge(combined_df, merra_df, on=\"Date\", how=\"outer\")\n",
    "\n",
    "# ------------------- ML & CLUSTERING -------------------\n",
    "features = ['Mean_AOD', 'PS', 'QV2M', 'T2M', 'TS', 'U10M', 'QV10M', 'SLP', 'T10M', 'T2MDEW', 'TQI', 'TQL']\n",
    "clean_df = combined_df.dropna(subset=features + ['PM2.5 (µg/m³)']).copy()\n",
    "clean_df['Temp_Diff'] = clean_df['TS'] - clean_df['T2M']\n",
    "clean_df['Humidity_Ratio'] = clean_df['QV2M'] / (clean_df['T2M'] + 1e-3)\n",
    "\n",
    "# ---------- CLUSTERING BY PM2.5 ----------\n",
    "num_clusters = 2   # you can choose more if you want\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clean_df['Cluster'] = kmeans.fit_predict(clean_df['PM2.5 (µg/m³)'].values.reshape(-1, 1))\n",
    "\n",
    "# ---------- PER-CLUSTER REGRESSION ----------\n",
    "cluster_models = {}\n",
    "\n",
    "for cluster_label in range(num_clusters):\n",
    "    cluster_data = clean_df[clean_df['Cluster'] == cluster_label]\n",
    "    # Features for model\n",
    "    X_clust = cluster_data[features].copy()\n",
    "    X_clust['Temp_Diff'] = cluster_data['Temp_Diff']\n",
    "    X_clust['Humidity_Ratio'] = cluster_data['Humidity_Ratio']\n",
    "    y_clust = cluster_data['PM2.5 (µg/m³)']\n",
    "\n",
    "    # Split and train\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_clust, y_clust, test_size=0.2, random_state=42)\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    cluster_models[cluster_label] = rf\n",
    "\n",
    "    # Evaluation\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(f\"\\n✅ Cluster {cluster_label} Evaluation:\")\n",
    "    print(f\"MAE: {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "    print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
    "    print(f\"R²: {r2_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# ---------------- PREDICTION FOR NEW DAY -------------------\n",
    "# Supply values for target day\n",
    "merra_file = \"merra_downloads/MERRA2_400.tavg1_2d_slv_Nx.20240124.SUB.nc\"  # <== Change for desired date\n",
    "merra_features = extract_merra_single_day(merra_file)\n",
    "merra_features[\"Mean_AOD\"] = [0.90]   # <== Change for desired date, or fetch from source\n",
    "\n",
    "predict_input_df = pd.DataFrame(merra_features)\n",
    "predict_input_df['Temp_Diff'] = predict_input_df['TS'] - predict_input_df['T2M']\n",
    "predict_input_df['Humidity_Ratio'] = predict_input_df['QV2M'] / (predict_input_df['T2M'] + 1e-3)\n",
    "X_input = predict_input_df[features].copy()\n",
    "X_input['Temp_Diff'] = predict_input_df['Temp_Diff']\n",
    "X_input['Humidity_Ratio'] = predict_input_df['Humidity_Ratio']\n",
    "\n",
    "# CLUSTER ASSIGNMENT for the new day\n",
    "# Option 1: Assign based on nearest cluster center by feature proxy (Mean_AOD, etc if available)\n",
    "# Option 2: Assign using mean PM2.5 value in clusters (if you expect pollution level) OR use cluster 0 or 1\n",
    "\n",
    "# Here we assign cluster with kmeans using a feature proxy. Since our clustering was on PM2.5,\n",
    "# and we don't have true PM2.5 for the day to cluster, just use both models and compare OR pick one.\n",
    "\n",
    "predictions = {}\n",
    "for cluster_label, model in cluster_models.items():\n",
    "    pred = model.predict(X_input)[0]\n",
    "    predictions[cluster_label] = pred\n",
    "\n",
    "print(\"\\n🔮 Cluster-wise Predictions for new day:\")\n",
    "for cluster_label, pred in predictions.items():\n",
    "    print(f\"Cluster {cluster_label}: Predicted PM2.5 = {pred:.2f}\")\n",
    "\n",
    "# You can pick the prediction you want, e.g., highest, lowest, or based on expected pollution scenario\n",
    "# Here let's pick the highest\n",
    "best_cluster = max(predictions, key=predictions.get)\n",
    "final_prediction = predictions[best_cluster]\n",
    "\n",
    "# --- extract date from filename (for output) ---\n",
    "match = re.search(r'(\\d{8})', merra_file)\n",
    "if match:\n",
    "    pred_date = pd.to_datetime(match.group(1), format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "else:\n",
    "    pred_date = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# ----- Save prediction -----\n",
    "output_df = pd.DataFrame({\n",
    "    'Date': [pred_date],\n",
    "    'Predicted_PM2.5': [final_prediction],\n",
    "    'Cluster_Used': [best_cluster]\n",
    "})\n",
    "output_df.to_csv('predicted_pm25.csv', index=False, mode='w')\n",
    "print(f\"\\n💾 Prediction saved to 'predicted_pm25.csv' for {pred_date}, cluster {best_cluster}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
